{
  "metainfo" : {
    "id" : "1",
    "language" : "scala",
    "fabricId" : "1010",
    "frontEndLanguage" : "python",
    "mode" : "batch",
    "interimMode" : "Full",
    "udfs" : {
      "language" : "scala",
      "udfs" : [ ]
    },
    "udafs" : {
      "language" : "scala",
      "code" : "package udfs\n\nimport org.apache.spark.sql.expressions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\n\n/**\n  * Here you can define your custom aggregate functions.\n  *\n  * Make sure to register your `udafs` in the register_udafs function below.\n  *\n  * Example:\n  *\n  * object GeometricMean extends UserDefinedAggregateFunction {\n  *   // This is the input fields for your aggregate function.\n  *   override def inputSchema: org.apache.spark.sql.types.StructType =\n  *     StructType(StructField(\"value\", DoubleType) :: Nil)\n  *\n  *   // This is the internal fields you keep for computing your aggregate.\n  *   override def bufferSchema: StructType = StructType(\n  *     StructField(\"count\", LongType) ::\n  *     StructField(\"product\", DoubleType) :: Nil\n  *   )\n  *\n  *   // This is the output type of your aggregatation function.\n  *   override def dataType: DataType = DoubleType\n  *\n  *   override def deterministic: Boolean = true\n  *\n  *   // This is the initial value for your buffer schema.\n  *   override def initialize(buffer: MutableAggregationBuffer): Unit = {\n  *     buffer(0) = 0L\n  *     buffer(1) = 1.0\n  *   }\n  *\n  *   // This is how to update your buffer schema given an input.\n  *   override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n  *     buffer(0) = buffer.getAs[Long](0) + 1\n  *     buffer(1) = buffer.getAs[Double](1) * input.getAs[Double](0)\n  *   }\n  *\n  *   // This is how to merge two objects with the bufferSchema type.\n  *   override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n  *     buffer1(0) = buffer1.getAs[Long](0) + buffer2.getAs[Long](0)\n  *     buffer1(1) = buffer1.getAs[Double](1) * buffer2.getAs[Double](1)\n  *   }\n  *\n  *   // This is where you output the final value, given the final value of your bufferSchema.\n  *   override def evaluate(buffer: Row): Any = {\n  *     math.pow(buffer.getDouble(1), 1.toDouble / buffer.getLong(0))\n  *   }\n  * }\n  *\n  */\n\n\nobject UDAFs {\n  /**\n    * Registers UDAFs with Spark SQL\n    */\n  def registerUDAFs(spark: SparkSession): Unit = {\n    /**\n      * Example:\n      *\n      * spark.udf.register(\"gm\", GeometricMean)\n      *\n      */\n\n\n  }\n}\n"
    },
    "configuration" : {
      "common" : {
        "type" : "record",
        "fields" : [ {
          "name" : "run_date",
          "kind" : {
            "type" : "string",
            "value" : "2022-07-22"
          },
          "optional" : false
        } ]
      },
      "fabrics" : {
        "recipes_fabric" : {
          "type" : "record",
          "fields" : [ ]
        },
        "anshuman" : {
          "type" : "record",
          "fields" : [ ]
        },
        "gamedayfabric" : {
          "type" : "record",
          "fields" : [ ]
        },
        "anshuman2" : {
          "type" : "record",
          "fields" : [ ]
        }
      }
    },
    "sparkConf" : [ ],
    "hadoopConf" : [ ],
    "codeMode" : "sparse",
    "buildSystem" : "maven",
    "externalDependencies" : [ ]
  },
  "connections" : [ {
    "id" : "g9-tkJlR3Twmn2oyJtwYe",
    "source" : "yt0Kyk0Fw2vfDwWvgwPol",
    "sourcePort" : "YZhRpQLMMeWnnAYbiu1ay",
    "target" : "BZYMDuL1Giz5OI_Cew1Rf",
    "targetPort" : "kdu7M9A4V9_D8XpdWz5XS"
  }, {
    "id" : "TLuR6dWhZlSz_sc9XXHU1",
    "source" : "BGnGX1MIcUQl3eapfr2gY",
    "sourcePort" : "Qt_HRMfmXwqbkTpSjUBNk",
    "target" : "0NaJ-hDjEE5-qU1FXHp1G",
    "targetPort" : "0pt2yrJ0v5W8kpPc72_ms"
  }, {
    "id" : "kwoT4reL_xWqnv_NHxsS8",
    "source" : "0NaJ-hDjEE5-qU1FXHp1G",
    "sourcePort" : "-oMhDMmMiA6b7U6KzeXM5",
    "target" : "8rPURr76JciV0CeVwu-4Z",
    "targetPort" : "4qYk6Q9RSN_hij_iiAc0D"
  }, {
    "id" : "7TFJxzzpb-FAX8cKY6M8S",
    "source" : "8rPURr76JciV0CeVwu-4Z",
    "sourcePort" : "aiAcOfw_BA5p_7ZCXmase",
    "target" : "ssBJOJyHiF5-MiVgXPK13",
    "targetPort" : "ABCKoXhrNF75vfVC20HS_"
  }, {
    "id" : "M-FFIFu-nVwGyH9Q9eapd",
    "source" : "ssBJOJyHiF5-MiVgXPK13",
    "sourcePort" : "D20X9yS0WHbvVFo0sW7ZP",
    "target" : "yt0Kyk0Fw2vfDwWvgwPol",
    "targetPort" : "88XYV88FP7RG5RPgePOdN"
  } ],
  "processes" : {
    "yt0Kyk0Fw2vfDwWvgwPol" : {
      "id" : "yt0Kyk0Fw2vfDwWvgwPol",
      "component" : "Script",
      "metadata" : {
        "label" : "dvr",
        "slug" : "dvr",
        "x" : -350,
        "y" : -482.5,
        "language" : "scala",
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "88XYV88FP7RG5RPgePOdN",
          "slug" : "in0"
        } ],
        "outputs" : [ {
          "id" : "YZhRpQLMMeWnnAYbiu1ay",
          "slug" : "out0"
        } ],
        "selectedInputFields" : [ ]
      },
      "properties" : {
        "script" : "if (in0.filter(col(\"balance\") < 0).count() > 0){\n    throw new RuntimeException(\"Found negative account balances\")\n}\nval out0 = in0\n    ",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "ssBJOJyHiF5-MiVgXPK13" : {
      "id" : "ssBJOJyHiF5-MiVgXPK13",
      "component" : "SchemaTransform",
      "metadata" : {
        "label" : "import_ts",
        "slug" : "import_ts",
        "x" : -521,
        "y" : -481.5,
        "language" : "scala",
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "ABCKoXhrNF75vfVC20HS_",
          "slug" : "in"
        } ],
        "outputs" : [ {
          "id" : "D20X9yS0WHbvVFo0sW7ZP",
          "slug" : "out"
        } ],
        "selectedInputFields" : [ ]
      },
      "properties" : {
        "columnsSelector" : [ ],
        "transformations" : [ {
          "kind" : "AddReplaceColumn",
          "AddReplaceColumn" : {
            "sourceColumn" : "import_ts",
            "expression" : {
              "format" : "python",
              "expression" : "current_timestamp()"
            }
          },
          "DropColumn" : {
            "sourceColumn" : ""
          },
          "RenameColumn" : {
            "sourceColumn" : "",
            "targetColumn" : ""
          }
        } ]
      }
    },
    "0NaJ-hDjEE5-qU1FXHp1G" : {
      "id" : "0NaJ-hDjEE5-qU1FXHp1G",
      "component" : "Script",
      "metadata" : {
        "label" : "null_check",
        "slug" : "null_check",
        "x" : -859,
        "y" : -482,
        "language" : "scala",
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "0pt2yrJ0v5W8kpPc72_ms",
          "slug" : "in0"
        } ],
        "outputs" : [ {
          "id" : "-oMhDMmMiA6b7U6KzeXM5",
          "slug" : "out0"
        } ],
        "selectedInputFields" : [ ]
      },
      "properties" : {
        "script" : "if (in0.filter(\n    col(\"acc_id\").isNull\n    or col(\"person_id\").isNull\n    or col(\"product_id\").isNull\n    or col(\"business_date\").isNull\n    or col(\"balance\").isNull\n    ).count()> 0){\n    throw new RuntimeException(\"Schema Validation Failed\")\n}\nval out0 = in0\nout0",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "8rPURr76JciV0CeVwu-4Z" : {
      "id" : "8rPURr76JciV0CeVwu-4Z",
      "component" : "Deduplicate",
      "metadata" : {
        "label" : "dedupe",
        "slug" : "dedupe",
        "x" : -700,
        "y" : -482.5,
        "language" : "scala",
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "4qYk6Q9RSN_hij_iiAc0D",
          "slug" : "in"
        } ],
        "outputs" : [ {
          "id" : "aiAcOfw_BA5p_7ZCXmase",
          "slug" : "out"
        } ],
        "selectedInputFields" : [ ]
      },
      "properties" : {
        "columnsSelector" : [ ],
        "dedupType" : "any",
        "dedupColumns" : [ {
          "colName" : "acc_id"
        }, {
          "colName" : "business_date"
        } ]
      }
    },
    "BZYMDuL1Giz5OI_Cew1Rf" : {
      "id" : "BZYMDuL1Giz5OI_Cew1Rf",
      "component" : "Target",
      "metadata" : {
        "label" : "acc_status_silver",
        "slug" : "acc_status_silver",
        "x" : -164,
        "y" : -485.5,
        "language" : "scala",
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "kdu7M9A4V9_D8XpdWz5XS",
          "slug" : "in"
        } ],
        "outputs" : [ ],
        "selectedInputFields" : [ ]
      },
      "properties" : {
        "datasetId" : "4088/datasets/acc_status_silver"
      }
    },
    "BGnGX1MIcUQl3eapfr2gY" : {
      "id" : "BGnGX1MIcUQl3eapfr2gY",
      "component" : "Source",
      "metadata" : {
        "label" : "acc_status_bronze",
        "slug" : "acc_status_bronze",
        "x" : -1062,
        "y" : -480,
        "language" : "scala",
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false
      },
      "ports" : {
        "inputs" : [ ],
        "outputs" : [ {
          "id" : "Qt_HRMfmXwqbkTpSjUBNk",
          "slug" : "out"
        } ],
        "selectedInputFields" : [ ]
      },
      "properties" : {
        "datasetId" : "4088/datasets/acc_status_bronze"
      }
    }
  },
  "ports" : {
    "inputs" : [ ],
    "outputs" : [ ],
    "selectedInputFields" : [ ]
  }
}